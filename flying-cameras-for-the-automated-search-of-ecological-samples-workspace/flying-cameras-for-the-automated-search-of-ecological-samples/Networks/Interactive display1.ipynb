{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive neural network display\n",
    "\n",
    "## Setting up\n",
    "#### 1. Importing Dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the networks\n",
    "#### 6. The 1st layer of the hyperspectral network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperspectral(input_,n_o_input, keep_prob, filter_width = 1, stride_size =1, relu_alpha = 0.2):\n",
    "    n_o_strides = int((n_o_input-filter_width)/stride_size) +1  #round down\n",
    "   \n",
    "    Hyper_layer = []\n",
    "    \n",
    "    def dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha):\n",
    "        nn_input = tf.slice(input_,[0,start],[-1,filter_width])\n",
    "        \n",
    "        dropout1 = tf.nn.dropout(nn_input, keep_prob)\n",
    "        dense1 = tf.layers.dense(dropout1, 1)\n",
    "        relu1 = tf.maximum(relu_alpha * dense1, dense1)        \n",
    "        return relu1\n",
    "    \n",
    "    for step in range(n_o_strides):\n",
    "        start = step*stride_size\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "    \n",
    "    if (n_o_input-filter_width)%stride_size>0:\n",
    "        start = n_o_input-filter_width\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "        \n",
    "    Hyper_l_stacked = tf.concat(Hyper_layer,1)\n",
    "    \n",
    "    print(\"Hyper_l_stacked\",Hyper_l_stacked)\n",
    "    return Hyper_l_stacked , n_o_strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The remaining neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2):\n",
    "    print(\"n_o_input\",n_o_input)\n",
    "    if n_o_input == 3:\n",
    "        is_RGB = True\n",
    "    elif n_o_input == 571:\n",
    "        is_RGB = False\n",
    "    else:\n",
    "        raise ValueError('A very specific bad thing happened.'+str(n_o_input))\n",
    "    \n",
    "    if is_RGB:\n",
    "        dense0 = tf.layers.dense(input_, 3)    \n",
    "        relu0 = tf.maximum(relu_alpha * dense0, dense0)\n",
    "        first_layer_out = tf.nn.dropout(relu0, keep_prob)\n",
    "    else:\n",
    "        print(input_,n_o_input, keep_prob)\n",
    "        first_layer_out,n_o_input= hyperspectral(input_,n_o_input, keep_prob, filter_width = 30, stride_size =1, relu_alpha = 0.2)\n",
    "\n",
    "    hidden_size = n_o_input*2/3\n",
    "    hidden_nodes = int(hidden_size)+1 # rounding\n",
    "    print(\"hidden size:\",str(hidden_nodes))\n",
    "    \n",
    "    \n",
    "    dense1 = tf.layers.dense(first_layer_out, hidden_nodes)    \n",
    "    relu1 = tf.maximum(relu_alpha * dense1, dense1)\n",
    "    dropout1 = tf.nn.dropout(relu1, keep_prob)\n",
    "    \n",
    "    \n",
    "    class_logits = tf.layers.dense(dropout1, n_o_class)    \n",
    "    \n",
    "    return class_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Function to define loss value for training\n",
    "Inspired by the aggregate loss scoring system from a GAN semi-supervised networ, an aggregated scoring system was structured to calculate the network’s loss value for training. This averaged the loss between the main class classification and the subclass classification; the main classes being manure vs non-manure, and the subclasses being the specific animal class or material type. And this aggregated scoring system aimed to teach the network the relationship between the groups of subclasses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(input_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = 1):\n",
    "    \n",
    "    n_o_class = m_class+n_class\n",
    "    \n",
    "    #raw output\n",
    "    logits= Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2)\n",
    "    subclass_softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    #Reduce outputs from 8 subclasses to 2 main classes\n",
    "    m_class_logit, n_class_logit = tf.split(logits, [m_class, n_class], 1)\n",
    "    m_class_logit1 =tf.reduce_sum(m_class_logit,1, keepdims =True) \n",
    "    n_class_logit1 =tf.reduce_sum(n_class_logit,1, keepdims =True) \n",
    "    main_class_logits = tf.concat([m_class_logit1, n_class_logit1], 1)\n",
    "    main_class_softmax = tf.nn.softmax(main_class_logits)\n",
    "\n",
    "    return subclass_softmax,main_class_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "#### 11. Defining the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e68cc3cbe1b4f50bdb3a7622c461f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Material:', options=('Select', 'Make your own', 'Average Amphibian Scat',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb8ae9b7e6e4be4af9689a8d298abd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "from ipywidgets import interactive, interact , FloatSlider,widgets,VBox,HBox,interactive_output,Layout,Output\n",
    "from IPython.display import display,HTML,clear_output\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "RGB = []\n",
    "hyperspectral = []\n",
    "\n",
    "box1 = Output(layout={'width': '25%','border': '1px solid black'})\n",
    "box2 = Output(layout={'width': '35%','border': '1px solid black' })\n",
    "box3 = Output(layout={'width': '30%', 'border': '1px solid black'})\n",
    "box4 = Output(layout={'width': '45%','border': '1px solid black' })\n",
    "box5 = Output(layout={'width': '45%','border': '1px solid black' })\n",
    "\n",
    "slider1 = FloatSlider(min=0, max=1, step=0.01, value=0.5,description='380nm',layout= Layout(width='99%'))\n",
    "slider2 = FloatSlider(min=0, max=1, step=0.01, value=0.5,description='450nm',layout= Layout(width='99%'))\n",
    "slider3 = FloatSlider(min=0, max=1, step=0.01, value=0.5,description='550nm',layout= Layout(width='99%'))\n",
    "slider4 = FloatSlider(min=0, max=1, step=0.01, value=0.5,description='650nm',layout= Layout(width='99%'))\n",
    "slider5 = FloatSlider(min=0, max=1, step=0.01, value=0.5,description='750nm',layout= Layout(width='99%'))\n",
    "slider6 = FloatSlider(min=0, max=1, step=0.01, value=0.5,description='850nm',layout= Layout(width='99%'))\n",
    "slider7 = FloatSlider(min=0, max=1, step=0.01, value=0.5,description='950nm',layout= Layout(width='99%'))\n",
    "\n",
    "def click(event):\n",
    "    classify(RGB,hyperspectral)\n",
    "    \n",
    "button = widgets.Button(\n",
    "    description='Run Classification',\n",
    "    layout={'width': '130px','margin':'0px 0px 0px 15%'}\n",
    ")\n",
    "button.on_click(click)\n",
    "\n",
    "\n",
    "slider_list = [slider1,slider2,slider3,slider4,slider5,slider6,slider7,button]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for slider in slider_list:\n",
    "    with box3:\n",
    "        display(slider)\n",
    "\n",
    "def classify(RGB,hyperspectral):\n",
    "    keep_probability = 1\n",
    "    def print_network_output(save_model_path,reflectance_data,n_o_input):\n",
    "        tf.reset_default_graph()\n",
    "        n_class = 3\n",
    "        m_class = 5\n",
    "        n_o_class = m_class+n_class\n",
    "        epochs = 10\n",
    "        keep_probability = 0.95\n",
    "        sub_scaling = 1 \n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        input_ = tf.placeholder(tf.float32,  [None,n_o_input],name = 'x')\n",
    "        subclass_softmax,main_class_softmax =softmax(input_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = sub_scaling) \n",
    "        clear_output(wait=True)\n",
    "        print('Loading...')\n",
    "        print_network_output(save_model_path,[hyperspectral],571)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "            loader.restore(sess, save_model_path)\n",
    "            subclass_softmax_p,main_class_softmax_p= sess.run([subclass_softmax,main_class_softmax], feed_dict = {input_:reflectance_data,keep_prob:keep_probability})\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print(\"Network Classification\")\n",
    "            print(\"Main Class\")\n",
    "            main_class_softmax_p  = main_class_softmax_p[0]\n",
    "            print(\" - \"+str(main_class_softmax_p[0]))\n",
    "            print(\" - \"+str(main_class_softmax_p[1]))\n",
    "            print(\"Subclass\")\n",
    "            subclass_softmax_p = subclass_softmax_p[0]\n",
    "            for subclass in subclass_softmax_p:\n",
    "                print(\" - \" +str(subclass))\n",
    "    print_network_output('../training_Hyperspectral',[hyperspectral],571)\n",
    "\n",
    "\"\"\"    with box4:\n",
    "        save_model_path = '../training_Hyperspectral'\n",
    "        print_network_output(save_model_path,[hyperspectral],571)\n",
    "    with box5:\n",
    "        save_model_path = '../training_rgb'\n",
    "        print_network_output(save_model_path,[RGB],3)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "def f(z,a,b,c,d,e,f,g):\n",
    "    if z == \"Select\":\n",
    "        for slider in slider_list:\n",
    "            slider.layout.display = 'none'\n",
    "        return 0\n",
    "    data_path =\"../Training data/averaged all.csv\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data.head()\n",
    "    target_fields ='Class'\n",
    "    data = data.drop([\"Class code\"],axis=1)\n",
    "    features0, targets0 = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    features, targets  = np.array(features0) , np.array(targets0)\n",
    "\n",
    "    dic = {}\n",
    "    for feature,target in zip(features, targets):\n",
    "        dic[target]=list(feature)\n",
    "    if z == \"Make your own\":\n",
    "        x= [380,450,550,650,750,850,950]\n",
    "        y = [a,b,c,d,e,f,g]\n",
    "        plt.scatter(x, y)\n",
    "        plt.plot(x, y, 'o')\n",
    "    else:\n",
    "        x= [x for x in range(380,951)]\n",
    "        y = dic[z.replace(\"Average \",\"\")]     \n",
    "        for slider in slider_list:\n",
    "            slider.layout.display = 'none'\n",
    "            \n",
    "    graph = UnivariateSpline(x, y, s=0)\n",
    "    xs = np.linspace(380, 950, 100)\n",
    "    ys = graph(xs)\n",
    "    \n",
    "    with box2:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlim(380, 950)\n",
    "        plt.plot(xs, ys)\n",
    "        plt.title(\"Reflectance spectrum\")\n",
    "        plt.show()\n",
    "        \n",
    "    global hyperspectral\n",
    "    global RGB\n",
    "    hyperspectral = [ min(max(round(float(graph(x)),7),0),1) for x in range(380,951)]\n",
    "    red =  min(max(round(float(graph(680)),4),0),1)\n",
    "    green =  min(max(round(float(graph(530)),4),0),1)\n",
    "    blue =  min(max(round(float(graph(465)),4),0),1)\n",
    "    RGB = [red,green,blue]\n",
    "    \n",
    "    with box1:\n",
    "        clear_output(wait=True)\n",
    "        red_rgb = int(red*255)\n",
    "        green_rgb = int(green*255)\n",
    "        blue_rgb = int(blue*255)\n",
    "        #TODO: align central \n",
    "        html = '<svg height=\"140\" width=\"100%\"><rect x=\"5%\" y=\"10px\" width=\"70%\" height=\"140\" fill=\"rgb('+ str(red_rgb)+\",\"+str(green_rgb)+\",\"+str(blue_rgb)+')\" /></svg>'\n",
    "        display(HTML(html))\n",
    "        print(\"R(680nm): \", red )\n",
    "        print(\"G(530nm): \", green)\n",
    "        print(\"B(465nm): \", blue)\n",
    "        \n",
    "    if z == \"Make your own\":\n",
    "        for slider in slider_list:\n",
    "                slider.layout.display = 'flex'\n",
    "    else:\n",
    "        with box4:\n",
    "            clear_output(wait=True)\n",
    "            classify(RGB,hyperspectral)\n",
    "\n",
    "\n",
    "        \n",
    "material = widgets.Dropdown(\n",
    "    options=[\"Select\",\n",
    "        \"Make your own\",\n",
    "          'Average Amphibian Scat', \n",
    "          'Average Bird Scat', \n",
    "          'Average Ground',\n",
    "          \"Average Leaf Litter\",\n",
    "          \"Average Mammal Scat\",\n",
    "          \"Average Reptile Scat\",\n",
    "          \"Average Reptile Urea\",\n",
    "          \"Average Wood\"],\n",
    "    value='Select',\n",
    "    description='Material:',\n",
    ")\n",
    "output = interactive_output(f, {\"z\":material,'a':slider1,'b' :slider2, 'c':slider3,'d':slider4,'e' :slider5, 'f':slider6,'g':slider7})\n",
    "hbox1 = HBox([box1,box2,box3])\n",
    "hbox2 = HBox([box4,box5])\n",
    "\n",
    "vbox3 = VBox([material,hbox1,hbox2])\n",
    "display(vbox3,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
