{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral #Trial 1\n",
    "\n",
    "A neural network is a computational tool that has to ability to easily learn complex patterns from a large quantity of data for applications such as classification and feature imitation.\n",
    "\n",
    "A neural network consists of input nodes and subsequent nodes that sum the value of nodes before it, with each node value multiplied by a scalar. Initially, the scalar values are randomly assigned. But a partial derivative can determine the influence of each node on the final output. The values of the scalars are then updated based on the influence/partial-derivative values and the sizes of the classification errors. Over time, this gradient descent process would improve the classification accuracy. With sufficient training data, neural networks can then be used to identify scat samples in the wild based on their hyperspectral spectra.\n",
    "\n",
    "## Setting up\n",
    "\n",
    "#### 1. Specifying Network specific variable\n",
    "Other than the variable in the next cell, the rest of the code is common for both a RGB and hyperspectral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '../training_Hyperspectral'\n",
    "data_path =\"../Training data/Hyperspectral.csv\"\n",
    "n_o_input = 571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Importing Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "#### 3. Importing data and splitting it into label and training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data.head()\n",
    "\n",
    "target_fields ='Class code'\n",
    "data = data.drop([\"Class\",\"Sample\",\"Session.Sample\"],axis=1)\n",
    "features0, targets0 = data.drop(target_fields, axis=1), data[target_fields]\n",
    "features, targets  = np.array(features0) , np.array(targets0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Function to generate a non-repeating list of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_no_repeat(lower,upper,length):\n",
    "    assert length<=(upper-lower)+1\n",
    "    output = []\n",
    "    while len(output)<length:\n",
    "        x = random.randint(lower,upper)\n",
    "        if not(x in output):\n",
    "            output.append(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pick spectral data from random sessions for testing and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_length = 1\n",
    "test_length = 1\n",
    "n_o_sample = 47\n",
    "size_of_a_sample = 7\n",
    "\n",
    "#Pick random labels for validation & test set\n",
    "val_labels = []\n",
    "test_labels = [] \n",
    "for num in range(n_o_sample):\n",
    "    rand_upper = size_of_a_sample-1\n",
    "    rand_len = test_length+validation_length\n",
    "    ran_list = random_no_repeat(0,rand_upper,rand_len)\n",
    "    \n",
    "    rand_valid = np.array(ran_list[0:validation_length])\n",
    "    val_labels.extend(n_o_sample*rand_valid+num)\n",
    "    \n",
    "    rand_t_start = validation_length\n",
    "    rand_t_end = validation_length+test_length\n",
    "    rand_test= np.array(ran_list[rand_t_start:rand_t_end])\n",
    "    test_labels.extend(n_o_sample*rand_test+num)\n",
    "\n",
    "#Labels for training set by removing those allocated to test & validation\n",
    "train_labels = list(\n",
    "    set([x for x in range(n_o_sample*size_of_a_sample)])\n",
    "    - set(val_labels)\n",
    "    - set(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 1\n",
    "Check that the random session index for each sample is pick correctly, where labels of validation and test set should be same for given sample number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[235] [282]\n",
      "[1] [1]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "print(num) #Sample number\n",
    "print(n_o_sample*rand_valid+num,n_o_sample*rand_test+num) #Index of sample from random trials\n",
    "print(targets[n_o_sample*rand_valid+num],targets[n_o_sample*rand_test+num]) #Lables of random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. The samples are ordered in related groups. Randomizing the data would remove biases from the sample's related order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(train_labels)\n",
    "shuffle(val_labels)\n",
    "shuffle(test_labels)\n",
    "train_x, train_y = features[train_labels] ,targets[train_labels]\n",
    "val_x , val_y = features[val_labels] , targets[val_labels]\n",
    "test_x, test_y = features[test_labels] ,targets[test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2\n",
    "Check that labels are in same order (disable the shuffle function first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 4 5 6 2 1 3 2 1 2 5 1 4 3 6 2 4 2 8 3 2 4 2 2 8 1 3 7 3 2 1 1 3 3 5\n",
      " 3 3 3 3 5 2 1 2 5 7]\n",
      "[7 5 1 6 3 4 2 3 4 3 2 1 5 2 2 5 3 6 3 3 1 1 2 2 1 4 3 2 4 8 2 3 1 5 1 2 4\n",
      " 1 2 3 5 2 3 2 7 3 8]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)\n",
    "print(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the networks\n",
    "#### 7. The 1st layer of the hyperspectral network\n",
    "For the hyperspectral data set, an additional layer was added to the start of the network topology to reduce the data’s complexity. Traditionally, the first hidden layer looks for simpler features, in this case, the individual absorption features. Looking for correlation at an individual wavelength level, however, requires much computational cost. But as the absorption features exist as clusters of neighbouring wavelengths, we can use a convolution network like approach to convey the sequential and incremental relationship of wavelength inputs.  The prioritization of relationships between neighbouring inputs would reduce the computational cost. This convolution network like structures was, however, built from scratch, by stacking smaller dense layers together. This was to avoid the spatial invariance, whose generalisation in absorption features characteristics would disregard their potential variance across the spectrum.\n",
    "  [simplify elaboration]\n",
    "\n",
    "\n",
    "A leaky RELU (Rectified linear unit) was also used in both networks as the activation function. This ensured the required non-linearity for problem-solving without the high computational cost of functions like softmax. It also prevented the exploding gradient in backpropagation that is associated with the conventional RELU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperspectral(input_,n_o_input, keep_prob, filter_width = 1, stride_size =1, relu_alpha = 0.2):\n",
    "    n_o_strides = int((n_o_input-filter_width)/stride_size) +1  #round down\n",
    "   \n",
    "    Hyper_layer = []\n",
    "    \n",
    "    def dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha):\n",
    "        nn_input = tf.slice(input_,[0,start],[-1,filter_width])\n",
    "        \n",
    "        dropout1 = tf.nn.dropout(nn_input, keep_prob)\n",
    "        dense1 = tf.layers.dense(dropout1, 1)\n",
    "        relu1 = tf.maximum(relu_alpha * dense1, dense1)        \n",
    "        return relu1\n",
    "    \n",
    "    for step in range(n_o_strides):\n",
    "        start = step*stride_size\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "    \n",
    "    if (n_o_input-filter_width)%stride_size>0:\n",
    "        start = n_o_input-filter_width\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "        \n",
    "    Hyper_l_stacked = tf.concat(Hyper_layer,1)\n",
    "    \n",
    "    print(\"Hyper_l_stacked\",Hyper_l_stacked)\n",
    "    return Hyper_l_stacked , n_o_strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. The remaining layers which are common for both the hyperspectral and RGB network\n",
    "Consisting of 2 layers, the remaining structures for the neural networks of the RGB and hyperspectral data set were similar. Both outputs were then consisted of 8 nodes, for the 8 total sub-classes of dropping and abiotic elements. For the droppings, the classes were segregated by the 5 type of excrement and the animal of origin. For the abiotic element, the classes were instead segregated by 3 material types. [See table ... from the report] For the 2 networks, the sizes of the hidden layer prior to the outputs were then both calculated by multiplying the layer prior with a ⅔ ratio, a structure that is common in neural network development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2):\n",
    "    print(\"n_o_input\",n_o_input)\n",
    "    if n_o_input == 3:\n",
    "        is_RGB = True\n",
    "    elif n_o_input == 571:\n",
    "        is_RGB = False\n",
    "    else:\n",
    "        raise ValueError('A very specific bad thing happened.'+str(n_o_input))\n",
    "    \n",
    "    if is_RGB:\n",
    "        dense0 = tf.layers.dense(input_, 3)    \n",
    "        relu0 = tf.maximum(relu_alpha * dense0, dense0)\n",
    "        first_layer_out = tf.nn.dropout(relu0, keep_prob)\n",
    "    else:\n",
    "        first_layer_out,n_o_input= hyperspectral(input_,n_o_input, keep_prob, filter_width = 30, stride_size =1, relu_alpha = 0.2)\n",
    "\n",
    "    hidden_size = n_o_input*2/3\n",
    "    hidden_nodes = int(hidden_size)+1 # rounding\n",
    "    print(\"hidden size:\",str(hidden_nodes))\n",
    "    \n",
    "    \n",
    "    dense1 = tf.layers.dense(first_layer_out, hidden_nodes)    \n",
    "    relu1 = tf.maximum(relu_alpha * dense1, dense1)\n",
    "    dropout1 = tf.nn.dropout(relu1, keep_prob)\n",
    "    \n",
    "    \n",
    "    class_logits = tf.layers.dense(dropout1, n_o_class)    \n",
    "    \n",
    "    return class_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up for training\n",
    "#### 9. Function to format neural network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_indv(x,n_o_class):    \n",
    "    output = np.zeros(n_o_class)\n",
    "    output[x-1]=1\n",
    "    return output\n",
    "\n",
    "def one_hot_encode(x,n_o_class):\n",
    "    output = []\n",
    "    for y in x:\n",
    "        output.append(one_hot_indv(y,n_o_class))\n",
    "        \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Function to split data into smaller batch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y,batch_size=10):\n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Function to define loss value for training\n",
    "Inspired by the aggregate loss scoring system from a GAN semi-supervised networ, an aggregated scoring system was structured to calculate the network’s loss value for training. This averaged the loss between the main class classification and the subclass classification; the main classes being manure vs non-manure, and the subclasses being the specific animal class or material type. And this aggregated scoring system aimed to teach the network the relationship between the groups of subclasses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_,target_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = 1):\n",
    "    \n",
    "    n_o_class = m_class+n_class\n",
    "    \n",
    "    #raw output\n",
    "    logits= Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2)\n",
    "    subclass_softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    #Reduce outputs from 8 subclasses to 2 main classes\n",
    "    n_class_logit, m_class_logit = tf.split(logits, [n_class, m_class], 1)\n",
    "    m_class_logit1 =tf.reduce_sum(m_class_logit,1, keepdims =True) \n",
    "    n_class_logit1 =tf.reduce_sum(n_class_logit,1, keepdims =True) \n",
    "    main_class_logits = tf.concat([n_class_logit1, m_class_logit1], 1)\n",
    "    main_class_softmax = tf.nn.softmax(main_class_logits)\n",
    "    \n",
    "    #Reduce labels from 8 subclasses to 2 main classes\n",
    "    n_class_label, m_class_label = tf.split(target_, [n_class, m_class], 1)\n",
    "    m_class_label1 =tf.reduce_sum(m_class_label,1, keepdims =True) \n",
    "    n_class_label1 =tf.reduce_sum(n_class_label,1, keepdims =True) \n",
    "    main_class_labels = tf.concat([n_class_label1, m_class_label1], 1)\n",
    "\n",
    "    #Aggregated cost\n",
    "    sub_class_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=target_))\n",
    "    main_class_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=main_class_logits, labels=main_class_labels))\n",
    "    total_cost = sub_class_cost + sub_scaling*main_class_cost\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(total_cost)\n",
    "    \n",
    "    #Accuracy value\n",
    "    subclass_accuracy = tf.equal(tf.argmax(logits, 1), tf.argmax(target_, 1))\n",
    "    subclass_accuracy =  tf.reduce_mean(tf.cast(subclass_accuracy, tf.float32), name='accuracy') #raw score \n",
    "    \n",
    "    main_class_accuracy = tf.equal(tf.argmax(main_class_logits, 1), tf.argmax(main_class_labels, 1))\n",
    "    main_class_accuracy = tf.reduce_mean(tf.cast(main_class_accuracy, tf.float32), name='accuracy') #raw score \n",
    "    \n",
    "    confidence_sub_class =  tf.reduce_sum(tf.multiply(subclass_softmax,target_),1)\n",
    "    confidence_main_class =  tf.reduce_sum(tf.multiply(main_class_softmax,main_class_labels),1)\n",
    "    \n",
    "    return optimizer,total_cost,subclass_softmax,main_class_softmax,subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "#### 12. Defining the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_o_input 571\n",
      "Hyper_l_stacked Tensor(\"concat:0\", shape=(?, 542), dtype=float32)\n",
      "hidden size: 362\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_class = 3\n",
    "m_class = 5\n",
    "n_o_class = m_class+n_class\n",
    "input_ = tf.placeholder(tf.float32,  [None,n_o_input],name = 'x')\n",
    "target_ = tf.placeholder(tf.float32,[None,n_o_class],name='y')\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "epochs = 120\n",
    "keep_probability = 0.95\n",
    "sub_scaling = 1 \n",
    "optimizer,total_cost,subclass_softmax,main_class_softmax,subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class =model_loss(input_,target_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = sub_scaling) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Doing the actual training\n",
    "The neural network was modified based on the classification accuracy for the validation set. These modifications are made to find the \"optimal\" structure with the best classifcation performance.  \n",
    "- Prediction accuracy represents that % of readings in each batch whose subclass is correctly classified  \n",
    "- Prediction confidence represents the cross entropy score. In statistics, a prediction at a 95% confidence would mean that the prediction would be correct for 95% of the time where such confidence value was given.  While the network’s classification confidence doesn’t directly translate to a statistical confidence, the confidence value of a  well trained neural network does exist as a good indication of future prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Validation Epoch  1 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.57507, Subclass Accuracy: 0.10000, Subclass Confidence: 0.12841\n",
      "Validation Epoch  2 - Main Class Accuracy: 0.40000, , Main Class Confidence 0.45470, Subclass Accuracy: 0.20000, Subclass Confidence: 0.15659\n",
      "Validation Epoch  3 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.61220, Subclass Accuracy: 0.20000, Subclass Confidence: 0.22289\n",
      "Validation Epoch  4 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.73168, Subclass Accuracy: 0.50000, Subclass Confidence: 0.34538\n",
      "Validation Epoch  5 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.78176, Subclass Accuracy: 0.30000, Subclass Confidence: 0.19574\n",
      "Validation Epoch  6 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.77232, Subclass Accuracy: 0.40000, Subclass Confidence: 0.28902\n",
      "Validation Epoch  7 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.77078, Subclass Accuracy: 0.30000, Subclass Confidence: 0.22703\n",
      "Validation Epoch  8 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.64860, Subclass Accuracy: 0.30000, Subclass Confidence: 0.22481\n",
      "Validation Epoch  9 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.77467, Subclass Accuracy: 0.40000, Subclass Confidence: 0.26715\n",
      "Validation Epoch 10 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.77941, Subclass Accuracy: 0.60000, Subclass Confidence: 0.42190\n",
      "Validation Epoch 11 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.80102, Subclass Accuracy: 0.60000, Subclass Confidence: 0.31317\n",
      "Validation Epoch 12 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.72773, Subclass Accuracy: 0.50000, Subclass Confidence: 0.30478\n",
      "Validation Epoch 13 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.71317, Subclass Accuracy: 0.60000, Subclass Confidence: 0.30831\n",
      "Validation Epoch 14 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.82622, Subclass Accuracy: 0.40000, Subclass Confidence: 0.24741\n",
      "Validation Epoch 15 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.75194, Subclass Accuracy: 0.40000, Subclass Confidence: 0.34431\n",
      "Validation Epoch 16 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.85732, Subclass Accuracy: 0.60000, Subclass Confidence: 0.34140\n",
      "Validation Epoch 17 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.71258, Subclass Accuracy: 0.50000, Subclass Confidence: 0.33228\n",
      "Validation Epoch 18 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.75399, Subclass Accuracy: 0.40000, Subclass Confidence: 0.34556\n",
      "Validation Epoch 19 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.85320, Subclass Accuracy: 0.60000, Subclass Confidence: 0.43330\n",
      "Validation Epoch 20 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.81226, Subclass Accuracy: 0.40000, Subclass Confidence: 0.35795\n",
      "Validation Epoch 21 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.74882, Subclass Accuracy: 0.40000, Subclass Confidence: 0.26585\n",
      "Validation Epoch 22 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.76823, Subclass Accuracy: 0.40000, Subclass Confidence: 0.34034\n",
      "Validation Epoch 23 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.69536, Subclass Accuracy: 0.40000, Subclass Confidence: 0.34335\n",
      "Validation Epoch 24 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.70583, Subclass Accuracy: 0.60000, Subclass Confidence: 0.34462\n",
      "Validation Epoch 25 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.73023, Subclass Accuracy: 0.40000, Subclass Confidence: 0.37886\n",
      "Validation Epoch 26 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.72451, Subclass Accuracy: 0.50000, Subclass Confidence: 0.38906\n",
      "Validation Epoch 27 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.94437, Subclass Accuracy: 0.50000, Subclass Confidence: 0.37850\n",
      "Validation Epoch 28 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88877, Subclass Accuracy: 0.40000, Subclass Confidence: 0.40927\n",
      "Validation Epoch 29 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.80469, Subclass Accuracy: 0.50000, Subclass Confidence: 0.32832\n",
      "Validation Epoch 30 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.71449, Subclass Accuracy: 0.20000, Subclass Confidence: 0.18949\n",
      "Validation Epoch 31 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88191, Subclass Accuracy: 0.70000, Subclass Confidence: 0.46297\n",
      "Validation Epoch 32 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.76192, Subclass Accuracy: 0.60000, Subclass Confidence: 0.36830\n",
      "Validation Epoch 33 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.84790, Subclass Accuracy: 0.50000, Subclass Confidence: 0.44471\n",
      "Validation Epoch 34 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.71132, Subclass Accuracy: 0.50000, Subclass Confidence: 0.40872\n",
      "Validation Epoch 35 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.90566, Subclass Accuracy: 0.50000, Subclass Confidence: 0.35844\n",
      "Validation Epoch 36 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.83863, Subclass Accuracy: 0.60000, Subclass Confidence: 0.39559\n",
      "Validation Epoch 37 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.83444, Subclass Accuracy: 0.50000, Subclass Confidence: 0.36984\n",
      "Validation Epoch 38 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.77149, Subclass Accuracy: 0.70000, Subclass Confidence: 0.43381\n",
      "Validation Epoch 39 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.73801, Subclass Accuracy: 0.40000, Subclass Confidence: 0.36490\n",
      "Validation Epoch 40 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.87455, Subclass Accuracy: 0.60000, Subclass Confidence: 0.42512\n",
      "Validation Epoch 41 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.72270, Subclass Accuracy: 0.50000, Subclass Confidence: 0.36322\n",
      "Validation Epoch 42 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.77887, Subclass Accuracy: 0.50000, Subclass Confidence: 0.43098\n",
      "Validation Epoch 43 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.83270, Subclass Accuracy: 0.40000, Subclass Confidence: 0.42337\n",
      "Validation Epoch 44 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88300, Subclass Accuracy: 0.50000, Subclass Confidence: 0.46676\n",
      "Validation Epoch 45 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.91269, Subclass Accuracy: 0.60000, Subclass Confidence: 0.58385\n",
      "Validation Epoch 46 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.94926, Subclass Accuracy: 0.60000, Subclass Confidence: 0.50402\n",
      "Validation Epoch 47 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.89277, Subclass Accuracy: 0.60000, Subclass Confidence: 0.41472\n",
      "Validation Epoch 48 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.61024, Subclass Accuracy: 0.30000, Subclass Confidence: 0.25895\n",
      "Validation Epoch 49 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.97600, Subclass Accuracy: 0.60000, Subclass Confidence: 0.52352\n",
      "Validation Epoch 50 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.80284, Subclass Accuracy: 0.70000, Subclass Confidence: 0.54538\n",
      "Validation Epoch 51 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.69127, Subclass Accuracy: 0.30000, Subclass Confidence: 0.28004\n",
      "Validation Epoch 52 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.89710, Subclass Accuracy: 0.60000, Subclass Confidence: 0.53557\n",
      "Validation Epoch 53 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88166, Subclass Accuracy: 0.70000, Subclass Confidence: 0.52840\n",
      "Validation Epoch 54 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.79763, Subclass Accuracy: 0.50000, Subclass Confidence: 0.38732\n",
      "Validation Epoch 55 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.78739, Subclass Accuracy: 0.50000, Subclass Confidence: 0.29948\n",
      "Validation Epoch 56 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.73697, Subclass Accuracy: 0.40000, Subclass Confidence: 0.36772\n",
      "Validation Epoch 57 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.91296, Subclass Accuracy: 0.70000, Subclass Confidence: 0.61604\n",
      "Validation Epoch 58 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.82551, Subclass Accuracy: 0.50000, Subclass Confidence: 0.51076\n",
      "Validation Epoch 59 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.73851, Subclass Accuracy: 0.50000, Subclass Confidence: 0.38680\n",
      "Validation Epoch 60 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88002, Subclass Accuracy: 0.60000, Subclass Confidence: 0.55552\n",
      "Validation Epoch 61 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.90475, Subclass Accuracy: 0.50000, Subclass Confidence: 0.41372\n",
      "Validation Epoch 62 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.84777, Subclass Accuracy: 0.50000, Subclass Confidence: 0.41248\n",
      "Validation Epoch 63 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.83524, Subclass Accuracy: 0.40000, Subclass Confidence: 0.31742\n",
      "Validation Epoch 64 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.80677, Subclass Accuracy: 0.30000, Subclass Confidence: 0.32453\n",
      "Validation Epoch 65 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.80231, Subclass Accuracy: 0.60000, Subclass Confidence: 0.47154\n",
      "Validation Epoch 66 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88142, Subclass Accuracy: 0.60000, Subclass Confidence: 0.48850\n",
      "Validation Epoch 67 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.93692, Subclass Accuracy: 0.70000, Subclass Confidence: 0.61265\n",
      "Validation Epoch 68 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.79294, Subclass Accuracy: 0.60000, Subclass Confidence: 0.36427\n",
      "Validation Epoch 69 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.77509, Subclass Accuracy: 0.40000, Subclass Confidence: 0.40908\n",
      "Validation Epoch 70 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.90388, Subclass Accuracy: 0.40000, Subclass Confidence: 0.46240\n",
      "Validation Epoch 71 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.84046, Subclass Accuracy: 0.40000, Subclass Confidence: 0.47934\n",
      "Validation Epoch 72 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.85344, Subclass Accuracy: 0.50000, Subclass Confidence: 0.48899\n",
      "Validation Epoch 73 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.89352, Subclass Accuracy: 0.50000, Subclass Confidence: 0.47234\n",
      "Validation Epoch 74 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.83557, Subclass Accuracy: 0.50000, Subclass Confidence: 0.50700\n",
      "Validation Epoch 75 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.84128, Subclass Accuracy: 0.70000, Subclass Confidence: 0.42657\n",
      "Validation Epoch 76 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.77517, Subclass Accuracy: 0.70000, Subclass Confidence: 0.45228\n",
      "Validation Epoch 77 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.82720, Subclass Accuracy: 0.70000, Subclass Confidence: 0.49518\n",
      "Validation Epoch 78 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.82640, Subclass Accuracy: 0.70000, Subclass Confidence: 0.57238\n",
      "Validation Epoch 79 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.96203, Subclass Accuracy: 0.60000, Subclass Confidence: 0.49746\n",
      "Validation Epoch 80 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.86854, Subclass Accuracy: 0.80000, Subclass Confidence: 0.65846\n",
      "Validation Epoch 81 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.82572, Subclass Accuracy: 0.40000, Subclass Confidence: 0.41749\n",
      "Validation Epoch 82 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.76050, Subclass Accuracy: 0.60000, Subclass Confidence: 0.47807\n",
      "Validation Epoch 83 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.98287, Subclass Accuracy: 0.80000, Subclass Confidence: 0.54889\n",
      "Validation Epoch 84 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.83314, Subclass Accuracy: 0.60000, Subclass Confidence: 0.46482\n",
      "Validation Epoch 85 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.79886, Subclass Accuracy: 0.80000, Subclass Confidence: 0.57010\n",
      "Validation Epoch 86 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.87356, Subclass Accuracy: 0.50000, Subclass Confidence: 0.41804\n",
      "Validation Epoch 87 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.94779, Subclass Accuracy: 0.80000, Subclass Confidence: 0.50792\n",
      "Validation Epoch 88 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.92719, Subclass Accuracy: 0.60000, Subclass Confidence: 0.56112\n",
      "Validation Epoch 89 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.91649, Subclass Accuracy: 0.60000, Subclass Confidence: 0.47646\n",
      "Validation Epoch 90 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.80296, Subclass Accuracy: 0.50000, Subclass Confidence: 0.41586\n",
      "Validation Epoch 91 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.86710, Subclass Accuracy: 0.50000, Subclass Confidence: 0.49555\n",
      "Validation Epoch 92 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.75053, Subclass Accuracy: 0.70000, Subclass Confidence: 0.52094\n",
      "Validation Epoch 93 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88226, Subclass Accuracy: 0.60000, Subclass Confidence: 0.49140\n",
      "Validation Epoch 94 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.88560, Subclass Accuracy: 0.80000, Subclass Confidence: 0.45363\n",
      "Validation Epoch 95 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.76864, Subclass Accuracy: 0.50000, Subclass Confidence: 0.42978\n",
      "Validation Epoch 96 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.92141, Subclass Accuracy: 0.80000, Subclass Confidence: 0.58460\n",
      "Validation Epoch 97 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.74041, Subclass Accuracy: 0.40000, Subclass Confidence: 0.41271\n",
      "Validation Epoch 98 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.81221, Subclass Accuracy: 0.60000, Subclass Confidence: 0.35336\n",
      "Validation Epoch 99 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.78722, Subclass Accuracy: 0.70000, Subclass Confidence: 0.59444\n",
      "Validation Epoch 100 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.99824, Subclass Accuracy: 0.70000, Subclass Confidence: 0.61074\n",
      "Validation Epoch 101 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.91769, Subclass Accuracy: 0.80000, Subclass Confidence: 0.54423\n",
      "Validation Epoch 102 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.64505, Subclass Accuracy: 0.70000, Subclass Confidence: 0.47190\n",
      "Validation Epoch 103 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.83743, Subclass Accuracy: 0.70000, Subclass Confidence: 0.45890\n",
      "Validation Epoch 104 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.95417, Subclass Accuracy: 0.80000, Subclass Confidence: 0.65923\n",
      "Validation Epoch 105 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.90294, Subclass Accuracy: 0.40000, Subclass Confidence: 0.45920\n",
      "Validation Epoch 106 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.86722, Subclass Accuracy: 0.50000, Subclass Confidence: 0.53544\n",
      "Validation Epoch 107 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.75806, Subclass Accuracy: 0.70000, Subclass Confidence: 0.54792\n",
      "Validation Epoch 108 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.80935, Subclass Accuracy: 0.50000, Subclass Confidence: 0.43925\n",
      "Validation Epoch 109 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.79255, Subclass Accuracy: 0.50000, Subclass Confidence: 0.44597\n",
      "Validation Epoch 110 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.83449, Subclass Accuracy: 0.70000, Subclass Confidence: 0.56649\n",
      "Validation Epoch 111 - Main Class Accuracy: 0.50000, , Main Class Confidence 0.53833, Subclass Accuracy: 0.50000, Subclass Confidence: 0.34829\n",
      "Validation Epoch 112 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.81458, Subclass Accuracy: 0.70000, Subclass Confidence: 0.64172\n",
      "Validation Epoch 113 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.82716, Subclass Accuracy: 0.50000, Subclass Confidence: 0.45365\n",
      "Validation Epoch 114 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.83723, Subclass Accuracy: 0.70000, Subclass Confidence: 0.44881\n",
      "Validation Epoch 115 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.97031, Subclass Accuracy: 0.60000, Subclass Confidence: 0.51349\n",
      "Validation Epoch 116 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.91036, Subclass Accuracy: 0.60000, Subclass Confidence: 0.45646\n",
      "Validation Epoch 117 - Main Class Accuracy: 0.80000, , Main Class Confidence 0.77099, Subclass Accuracy: 0.50000, Subclass Confidence: 0.42879\n",
      "Validation Epoch 118 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.89337, Subclass Accuracy: 0.70000, Subclass Confidence: 0.52327\n",
      "Validation Epoch 119 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.98451, Subclass Accuracy: 0.70000, Subclass Confidence: 0.57126\n",
      "Validation Epoch 120 - Main Class Accuracy: 0.90000, , Main Class Confidence 0.87711, Subclass Accuracy: 0.60000, Subclass Confidence: 0.43976\n"
     ]
    }
   ],
   "source": [
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, label_pre_one_hot in get_batches(train_x, train_y,batch_size = 10):\n",
    "            batch_labels = one_hot_encode(label_pre_one_hot,n_o_class)\n",
    "            \n",
    "            optimizer_p= sess.run([optimizer], feed_dict = {input_:batch_features,target_:batch_labels,keep_prob:keep_probability})\n",
    "\n",
    "            batch_i += 1 \n",
    "        print('Validation Epoch {:>2} - '.format(epoch + 1), end='')\n",
    "        \n",
    "        random_index = random_no_repeat(0,len(val_x)-1,10)\n",
    "        valid_labels = one_hot_encode(val_y[random_index],n_o_class) \n",
    "        subclass_accuracy_p,main_class_accuracy_p,confidence_sub_class_p,confidence_main_class_p= sess.run([subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class], feed_dict = {input_:val_x[random_index],target_:valid_labels,keep_prob:1})\n",
    "        print(\"Main Class Accuracy: {:.5f}, , Main Class Confidence {:.5f}, Subclass Accuracy: {:.5f}, Subclass Confidence: {:.5f}\".format(np.mean(main_class_accuracy_p),np.mean(confidence_main_class_p),np.mean(subclass_accuracy_p),np.mean(confidence_sub_class_p)))\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "#### 13. Testing the neural network\n",
    "When the modifications made were no longer improving the validation accuracy, we were satisfied that we had achieved the \"optimal\" structure. \n",
    "The network was then tested with the an unseen set of spectral data to confirm network's performance. \n",
    "In the validation phase, modifications were made based on the network's previous performance , where some of the improvements may be specific to the validation data set. \n",
    "In the testing phase, the use of unseen data produced accuracy values that are representative of real-world performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from ../training_Hyperspectral\n",
      "Sample 1, Class 7,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99966, Subclass Accuracy: 0.00000, Subclass Confidence: 0.23126\n",
      "Sample 2, Class 5,Main Class Accuracy: 1.00000, Main Class Confidence: 0.98365, Subclass Accuracy: 1.00000, Subclass Confidence: 0.27240\n",
      "Sample 3, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 1.00000, Subclass Confidence: 0.66087\n",
      "Sample 4, Class 6,Main Class Accuracy: 1.00000, Main Class Confidence: 0.53258, Subclass Accuracy: 0.00000, Subclass Confidence: 0.03473\n",
      "Sample 5, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99999, Subclass Accuracy: 0.00000, Subclass Confidence: 0.07589\n",
      "Sample 6, Class 4,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99835, Subclass Accuracy: 1.00000, Subclass Confidence: 0.73946\n",
      "Sample 7, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99833, Subclass Accuracy: 1.00000, Subclass Confidence: 0.92019\n",
      "Sample 8, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.53739, Subclass Accuracy: 1.00000, Subclass Confidence: 0.34221\n",
      "Sample 9, Class 4,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 1.00000, Subclass Confidence: 0.98896\n",
      "Sample 10, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 1.00000, Subclass Confidence: 0.99797\n",
      "Sample 11, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.89960, Subclass Accuracy: 1.00000, Subclass Confidence: 0.78569\n",
      "Sample 12, Class 1,Main Class Accuracy: 0.00000, Main Class Confidence: 0.33534, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12936\n",
      "Sample 13, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.31915, Subclass Accuracy: 1.00000, Subclass Confidence: 0.38747\n",
      "Sample 14, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.97218, Subclass Accuracy: 1.00000, Subclass Confidence: 0.78377\n",
      "Sample 15, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99338, Subclass Accuracy: 1.00000, Subclass Confidence: 0.89486\n",
      "Sample 16, Class 5,Main Class Accuracy: 1.00000, Main Class Confidence: 0.90830, Subclass Accuracy: 1.00000, Subclass Confidence: 0.59198\n",
      "Sample 17, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99999, Subclass Accuracy: 1.00000, Subclass Confidence: 0.49682\n",
      "Sample 18, Class 6,Main Class Accuracy: 0.00000, Main Class Confidence: 0.27561, Subclass Accuracy: 0.00000, Subclass Confidence: 0.08022\n",
      "Sample 19, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.98706, Subclass Accuracy: 1.00000, Subclass Confidence: 0.57185\n",
      "Sample 20, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 0.00000, Subclass Confidence: 0.36867\n",
      "Sample 21, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99448, Subclass Accuracy: 1.00000, Subclass Confidence: 0.49885\n",
      "Sample 22, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99740, Subclass Accuracy: 0.00000, Subclass Confidence: 0.24147\n",
      "Sample 23, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.66932, Subclass Accuracy: 1.00000, Subclass Confidence: 0.26164\n",
      "Sample 24, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 1.00000, Subclass Confidence: 0.60571\n",
      "Sample 25, Class 1,Main Class Accuracy: 0.00000, Main Class Confidence: 0.07960, Subclass Accuracy: 0.00000, Subclass Confidence: 0.27862\n",
      "Sample 26, Class 4,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99995, Subclass Accuracy: 1.00000, Subclass Confidence: 0.98610\n",
      "Sample 27, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.90473, Subclass Accuracy: 1.00000, Subclass Confidence: 0.48990\n",
      "Sample 28, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.93071, Subclass Accuracy: 1.00000, Subclass Confidence: 0.72363\n",
      "Sample 29, Class 4,Main Class Accuracy: 1.00000, Main Class Confidence: 0.79000, Subclass Accuracy: 0.00000, Subclass Confidence: 0.27891\n",
      "Sample 30, Class 8,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 1.00000, Subclass Confidence: 1.00000\n",
      "Sample 31, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.91962, Subclass Accuracy: 1.00000, Subclass Confidence: 0.33674\n",
      "Sample 32, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99602, Subclass Accuracy: 0.00000, Subclass Confidence: 0.29339\n",
      "Sample 33, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.96030, Subclass Accuracy: 1.00000, Subclass Confidence: 0.55338\n",
      "Sample 34, Class 5,Main Class Accuracy: 1.00000, Main Class Confidence: 0.66860, Subclass Accuracy: 0.00000, Subclass Confidence: 0.34256\n",
      "Sample 35, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99970, Subclass Accuracy: 0.00000, Subclass Confidence: 0.17529\n",
      "Sample 36, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99999, Subclass Accuracy: 1.00000, Subclass Confidence: 0.43827\n",
      "Sample 37, Class 4,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99901, Subclass Accuracy: 1.00000, Subclass Confidence: 0.86193\n",
      "Sample 38, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99924, Subclass Accuracy: 0.00000, Subclass Confidence: 0.28014\n",
      "Sample 39, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99954, Subclass Accuracy: 0.00000, Subclass Confidence: 0.05859\n",
      "Sample 40, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 1.00000, Subclass Confidence: 0.74292\n",
      "Sample 41, Class 5,Main Class Accuracy: 1.00000, Main Class Confidence: 0.65157, Subclass Accuracy: 1.00000, Subclass Confidence: 0.47931\n",
      "Sample 42, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99808, Subclass Accuracy: 1.00000, Subclass Confidence: 0.97648\n",
      "Sample 43, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99943, Subclass Accuracy: 1.00000, Subclass Confidence: 0.65395\n",
      "Sample 44, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99542, Subclass Accuracy: 1.00000, Subclass Confidence: 0.94831\n",
      "Sample 45, Class 7,Main Class Accuracy: 0.00000, Main Class Confidence: 0.04691, Subclass Accuracy: 0.00000, Subclass Confidence: 0.02052\n",
      "Sample 46, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.99844, Subclass Accuracy: 1.00000, Subclass Confidence: 0.95359\n",
      "Sample 47, Class 8,Main Class Accuracy: 1.00000, Main Class Confidence: 1.00000, Subclass Accuracy: 1.00000, Subclass Confidence: 1.00000\n"
     ]
    }
   ],
   "source": [
    "keep_probability = 1\n",
    "print('Testing...')\n",
    "with tf.Session() as sess:\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        batch_i = 1\n",
    "        for batch_features, label_pre_one_hot in get_batches(val_x, val_y,batch_size = 1):# to be changed\n",
    "            batch_labels = one_hot_encode(label_pre_one_hot,n_o_class)      \n",
    "            \n",
    "            subclass_accuracy_p,main_class_accuracy_p,confidence_sub_class_p,confidence_main_class_p= sess.run([subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class], feed_dict = {input_:batch_features,target_:batch_labels,keep_prob:1})\n",
    "            print('Sample {}, Class {},'.format(batch_i,label_pre_one_hot[0]), end='')\n",
    "            print(\"Main Class Accuracy: {:.5f}, Main Class Confidence: {:.5f}, Subclass Accuracy: {:.5f}, Subclass Confidence: {:.5f}\".format(np.mean(main_class_accuracy_p),np.mean(confidence_main_class_p),np.mean(subclass_accuracy_p),np.mean(confidence_sub_class_p)))\n",
    "\n",
    "            batch_i += 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
